{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nfrom torch.nn import functional as F\n\nfrom transformers import (BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction,\n                        AutoModelForQuestionAnswering,AutoTokenizer, AutoModelForSeq2SeqLM, \n                        LEDTokenizer, LEDForConditionalGeneration)\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:35:19.706349Z","iopub.execute_input":"2024-12-19T05:35:19.706745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:26.382649Z","iopub.execute_input":"2024-12-18T19:46:26.383307Z","iopub.status.idle":"2024-12-18T19:46:27.955905Z","shell.execute_reply.started":"2024-12-18T19:46:26.383268Z","shell.execute_reply":"2024-12-18T19:46:27.954398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Encoding","metadata":{}},{"cell_type":"code","source":"text = '08:59: waiting for my team to join the call'\nencoding = tokenizer.encode_plus(text, add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\nencoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:27.957549Z","iopub.execute_input":"2024-12-18T19:46:27.957917Z","iopub.status.idle":"2024-12-18T19:46:28.024151Z","shell.execute_reply.started":"2024-12-18T19:46:27.957876Z","shell.execute_reply":"2024-12-18T19:46:28.023074Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Masked","metadata":{}},{"cell_type":"code","source":"masked_model = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:28.025734Z","iopub.execute_input":"2024-12-18T19:46:28.026046Z","iopub.status.idle":"2024-12-18T19:46:30.809074Z","shell.execute_reply.started":"2024-12-18T19:46:28.026016Z","shell.execute_reply":"2024-12-18T19:46:30.807908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = \"The Opera House in Australia is in , \" + tokenizer.mask_token + \" city\"\n\ninput = tokenizer.encode_plus(text, return_tensors = \"pt\")\nmask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:30.811898Z","iopub.execute_input":"2024-12-18T19:46:30.812279Z","iopub.status.idle":"2024-12-18T19:46:30.829879Z","shell.execute_reply.started":"2024-12-18T19:46:30.812242Z","shell.execute_reply":"2024-12-18T19:46:30.828608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = masked_model(**input)\nsoftmax = F.softmax(output.logits, dim = -1)\nmask_word = softmax[0, mask_index, :]\ntop_10 = torch.topk(mask_word, 3, dim = 1)[1][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:30.831062Z","iopub.execute_input":"2024-12-18T19:46:30.831362Z","iopub.status.idle":"2024-12-18T19:46:31.068697Z","shell.execute_reply.started":"2024-12-18T19:46:30.831332Z","shell.execute_reply":"2024-12-18T19:46:31.067657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for token in top_10:\n   word = tokenizer.decode([token])\n   new_sentence = text.replace(tokenizer.mask_token, word)\n   print(new_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:31.069928Z","iopub.execute_input":"2024-12-18T19:46:31.070272Z","iopub.status.idle":"2024-12-18T19:46:31.077297Z","shell.execute_reply.started":"2024-12-18T19:46:31.070238Z","shell.execute_reply":"2024-12-18T19:46:31.076025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Next sentence prediction","metadata":{}},{"cell_type":"code","source":"nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:31.078861Z","iopub.execute_input":"2024-12-18T19:46:31.079190Z","iopub.status.idle":"2024-12-18T19:46:31.368565Z","shell.execute_reply.started":"2024-12-18T19:46:31.079158Z","shell.execute_reply":"2024-12-18T19:46:31.367298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"Incredible journey, Isha! Your dedication and teamwork shine through this experience.\"\n\nnext_sentence = \"It's inspiring to see how you're using technology for such a meaningful cause.\"\n\nencoding = tokenizer.encode_plus(prompt, next_sentence, return_tensors='pt')\noutputs = nsp_model(**encoding)[0]\nF.softmax(outputs, dim = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:31.370052Z","iopub.execute_input":"2024-12-18T19:46:31.370541Z","iopub.status.idle":"2024-12-18T19:46:31.556806Z","shell.execute_reply.started":"2024-12-18T19:46:31.370486Z","shell.execute_reply":"2024-12-18T19:46:31.555585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"Incredible journey, Isha! Your dedication and teamwork shine through this experience.\"\n\nnext_sentence = \"80% of chronic diseases are preventable. \"\n\nencoding = tokenizer.encode_plus(prompt, next_sentence, return_tensors='pt')\noutputs = nsp_model(**encoding)[0]\nF.softmax(outputs, dim = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:31.558442Z","iopub.execute_input":"2024-12-18T19:46:31.558778Z","iopub.status.idle":"2024-12-18T19:46:31.667972Z","shell.execute_reply.started":"2024-12-18T19:46:31.558747Z","shell.execute_reply":"2024-12-18T19:46:31.666846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Question Answer\n### barely acceptable ","metadata":{}},{"cell_type":"code","source":"model_name = \"deepset/bert-base-cased-squad2\"\nqa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\nqa_tokeniser = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:31.669271Z","iopub.execute_input":"2024-12-18T19:46:31.669612Z","iopub.status.idle":"2024-12-18T19:46:35.204998Z","shell.execute_reply.started":"2024-12-18T19:46:31.669577Z","shell.execute_reply":"2024-12-18T19:46:35.204001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context = \"My name is Clara and I live in Berkeley.\"\n\nquestion = \"Where do I live?\"\n\n# We can use our tokenizer to automatically generate 2 sentence by passing the\n# two sequences to tokenizer as two arguments\ntokenized_inputs = qa_tokeniser(question, context, return_tensors=\"pt\")\ntokenized_inputs\n\nwith torch.no_grad():\n    outputs = qa_model(**tokenized_inputs)\n\nanswer_start_index = outputs.start_logits.argmax()\nanswer_end_index = outputs.end_logits.argmax()\n\n''' start_logits (torch.FloatTensor of shape (batch_size, sequence_length)) — Span-start scores (before SoftMax).\n\nend_logits (torch.FloatTensor of shape (batch_size, sequence_length)) — Span-end scores (before SoftMax). '''\n\npredict_answer_tokens = tokenized_inputs.input_ids[0, answer_start_index : answer_end_index + 1]\nqa_tokeniser.decode(predict_answer_tokens)\n     \n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:35.206197Z","iopub.execute_input":"2024-12-18T19:46:35.206575Z","iopub.status.idle":"2024-12-18T19:46:35.316934Z","shell.execute_reply.started":"2024-12-18T19:46:35.206541Z","shell.execute_reply":"2024-12-18T19:46:35.315597Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bert Text Generation (Dual Bert Architecture)\n### Don't Use it\n","metadata":{}},{"cell_type":"code","source":"b2b_tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:46:35.318517Z","iopub.execute_input":"2024-12-18T19:46:35.318888Z","iopub.status.idle":"2024-12-18T19:47:51.233441Z","shell.execute_reply.started":"2024-12-18T19:46:35.318852Z","shell.execute_reply":"2024-12-18T19:47:51.232204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"discofuse = \"\"\"As a run-blocker, Zeitler moves relatively well. Zeitler often struggles at the point of contact in space.\"\"\"\n\ninput_ids = b2b_tokenizer(discofuse, return_tensors=\"pt\").input_ids\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:47:51.237827Z","iopub.execute_input":"2024-12-18T19:47:51.238270Z","iopub.status.idle":"2024-12-18T19:47:59.645497Z","shell.execute_reply.started":"2024-12-18T19:47:51.238228Z","shell.execute_reply":"2024-12-18T19:47:59.644236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}