{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.nn import functional as F\n\nfrom transformers import (BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction,\n                        AutoModelForQuestionAnswering,AutoTokenizer, AutoModelForSeq2SeqLM, \n                        LEDTokenizer, LEDForConditionalGeneration)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:15.602735Z","iopub.execute_input":"2024-12-19T05:36:15.603906Z","iopub.status.idle":"2024-12-19T05:36:15.610970Z","shell.execute_reply.started":"2024-12-19T05:36:15.603849Z","shell.execute_reply":"2024-12-19T05:36:15.610078Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:16.619691Z","iopub.execute_input":"2024-12-19T05:36:16.620240Z","iopub.status.idle":"2024-12-19T05:36:16.752333Z","shell.execute_reply.started":"2024-12-19T05:36:16.620188Z","shell.execute_reply":"2024-12-19T05:36:16.751242Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Encoding","metadata":{}},{"cell_type":"code","source":"text = '08:59: waiting for my team to join the call'\nencoding = tokenizer.encode_plus(text, add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\nencoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:16.991577Z","iopub.execute_input":"2024-12-19T05:36:16.991990Z","iopub.status.idle":"2024-12-19T05:36:17.008634Z","shell.execute_reply.started":"2024-12-19T05:36:16.991953Z","shell.execute_reply":"2024-12-19T05:36:17.007482Z"},"scrolled":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 101, 5511, 1024, 5354, 1024, 3403, 2005, 2026, 2136, 2000, 3693, 1996,\n         2655,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Masked","metadata":{}},{"cell_type":"code","source":"masked_model = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:17.363278Z","iopub.execute_input":"2024-12-19T05:36:17.363961Z","iopub.status.idle":"2024-12-19T05:36:17.899045Z","shell.execute_reply.started":"2024-12-19T05:36:17.363923Z","shell.execute_reply":"2024-12-19T05:36:17.898118Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"text = \"The Opera House in Australia is in , \" + tokenizer.mask_token + \" city\"\n\ninput = tokenizer.encode_plus(text, return_tensors = \"pt\")\nmask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:17.901565Z","iopub.execute_input":"2024-12-19T05:36:17.901904Z","iopub.status.idle":"2024-12-19T05:36:17.920481Z","shell.execute_reply.started":"2024-12-19T05:36:17.901873Z","shell.execute_reply":"2024-12-19T05:36:17.919497Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"output = masked_model(**input)\nsoftmax = F.softmax(output.logits, dim = -1)\nmask_word = softmax[0, mask_index, :]\ntop_10 = torch.topk(mask_word, 3, dim = 1)[1][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:18.865075Z","iopub.execute_input":"2024-12-19T05:36:18.865473Z","iopub.status.idle":"2024-12-19T05:36:19.098311Z","shell.execute_reply.started":"2024-12-19T05:36:18.865438Z","shell.execute_reply":"2024-12-19T05:36:19.097312Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"for token in top_10:\n   word = tokenizer.decode([token])\n   new_sentence = text.replace(tokenizer.mask_token, word)\n   print(new_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:19.231176Z","iopub.execute_input":"2024-12-19T05:36:19.231573Z","iopub.status.idle":"2024-12-19T05:36:19.238910Z","shell.execute_reply.started":"2024-12-19T05:36:19.231538Z","shell.execute_reply":"2024-12-19T05:36:19.237695Z"}},"outputs":[{"name":"stdout","text":"The Opera House in Australia is in , sydney city\nThe Opera House in Australia is in , melbourne city\nThe Opera House in Australia is in , brisbane city\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Next sentence prediction","metadata":{}},{"cell_type":"code","source":"nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:20.826735Z","iopub.execute_input":"2024-12-19T05:36:20.827204Z","iopub.status.idle":"2024-12-19T05:36:20.991593Z","shell.execute_reply.started":"2024-12-19T05:36:20.827164Z","shell.execute_reply":"2024-12-19T05:36:20.990180Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"prompt = \"Incredible journey, Isha! Your dedication and teamwork shine through this experience.\"\n\nnext_sentence = \"It's inspiring to see how you're using technology for such a meaningful cause.\"\n\nencoding = tokenizer.encode_plus(prompt, next_sentence, return_tensors='pt')\noutputs = nsp_model(**encoding)[0]\nF.softmax(outputs, dim = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:21.264815Z","iopub.execute_input":"2024-12-19T05:36:21.265224Z","iopub.status.idle":"2024-12-19T05:36:21.457926Z","shell.execute_reply.started":"2024-12-19T05:36:21.265189Z","shell.execute_reply":"2024-12-19T05:36:21.456791Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([[9.9999e-01, 8.2553e-06]], grad_fn=<SoftmaxBackward0>)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"prompt = \"Incredible journey, Isha! Your dedication and teamwork shine through this experience.\"\n\nnext_sentence = \"80% of chronic diseases are preventable. \"\n\nencoding = tokenizer.encode_plus(prompt, next_sentence, return_tensors='pt')\noutputs = nsp_model(**encoding)[0]\nF.softmax(outputs, dim = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:21.920245Z","iopub.execute_input":"2024-12-19T05:36:21.920648Z","iopub.status.idle":"2024-12-19T05:36:22.048812Z","shell.execute_reply.started":"2024-12-19T05:36:21.920614Z","shell.execute_reply":"2024-12-19T05:36:22.047576Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([[6.3895e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Question Answer\n### barely acceptable ","metadata":{}},{"cell_type":"code","source":"model_name = \"deepset/bert-base-cased-squad2\"\nqa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\nqa_tokeniser = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:23.581911Z","iopub.execute_input":"2024-12-19T05:36:23.582903Z","iopub.status.idle":"2024-12-19T05:36:26.580964Z","shell.execute_reply.started":"2024-12-19T05:36:23.582853Z","shell.execute_reply":"2024-12-19T05:36:26.579438Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966bf5daa7bd46c1b76705181288eb47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76f8bfad1ddb449b89d505f903a6ef19"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/152 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7946be816d4b494a94edd18e12829e52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de2bde96a6c4223a24a5004e37f7d7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb3a3490fc67410d89cbe7c7777bd51c"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"context = \"My name is Clara and I live in Berkeley.\"\n\nquestion = \"Where do I live?\"\n\n# We can use our tokenizer to automatically generate 2 sentence by passing the\n# two sequences to tokenizer as two arguments\ntokenized_inputs = qa_tokeniser(question, context, return_tensors=\"pt\")\ntokenized_inputs\n\nwith torch.no_grad():\n    outputs = qa_model(**tokenized_inputs)\n\nanswer_start_index = outputs.start_logits.argmax()\nanswer_end_index = outputs.end_logits.argmax()\n\n''' start_logits (torch.FloatTensor of shape (batch_size, sequence_length)) — Span-start scores (before SoftMax).\n\nend_logits (torch.FloatTensor of shape (batch_size, sequence_length)) — Span-end scores (before SoftMax). '''\n\npredict_answer_tokens = tokenized_inputs.input_ids[0, answer_start_index : answer_end_index + 1]\nqa_tokeniser.decode(predict_answer_tokens)\n     \n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:26.582984Z","iopub.execute_input":"2024-12-19T05:36:26.583407Z","iopub.status.idle":"2024-12-19T05:36:26.687849Z","shell.execute_reply.started":"2024-12-19T05:36:26.583371Z","shell.execute_reply":"2024-12-19T05:36:26.686781Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'Berkeley'"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"## Bert Text Generation (Dual Bert Architecture)\n### Don't Use it\n","metadata":{}},{"cell_type":"code","source":"b2b_tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:36:26.689277Z","iopub.execute_input":"2024-12-19T05:36:26.689727Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6782b5d6c684a718386bebbdae4e164"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/846k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46c5dbea74584240b91e4c3597a354df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70696cf9a8a491d977570db1432eea5"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"discofuse = \"\"\"As a run-blocker, Zeitler moves relatively well. Zeitler often struggles at the point of contact in space.\"\"\"\n\ninput_ids = b2b_tokenizer(discofuse, return_tensors=\"pt\").input_ids\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}